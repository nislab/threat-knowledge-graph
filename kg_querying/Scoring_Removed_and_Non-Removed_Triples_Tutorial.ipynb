{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad76164",
   "metadata": {},
   "source": [
    "# Scoring Removed and Non-Removed Triples Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ccafd",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to document my attempts to be able to seperate removed and non-removed triples via the ampligraph package's Knowledge Graph embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80877947",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41b092",
   "metadata": {},
   "source": [
    "This tutorial was done with Python 3.7 and AmpliGraph version 1.4.0 installed. First, we will need to import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ampligraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535955e0",
   "metadata": {},
   "source": [
    "Futhermore, we will need several `.csv` files of our Knowledge Graph triples. These files can be found in the original Knowledge Graph tutorial GitHub page at https://github.com/nislab/threat-knowledge-graph/tree/main. The particular ones used for this tutorial were `cpe2cve-aug2021.csv`, `cve2cwe-aug2021.csv`, `cpe2cve-nov2022.csv`, `cve2cwe-nov2022.csv`, and `kg_demo_aug2021.csv`.\n",
    "\n",
    "Finally, we will need the trained embedding models from the Ampligraph package. This tutorial will use the TransE embedding model. To train a model, we can run the following block of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "model_transe = TransE(batches_count=50,\n",
    "                epochs=300,\n",
    "                k=100,\n",
    "                eta=20,\n",
    "                embedding_model_params={'corrupt_sides': ['s,o'], 'negative_corruption_entities': 'batch'},\n",
    "                optimizer='adam',\n",
    "                optimizer_params={'lr':1e-4},\n",
    "                loss='multiclass_nll',\n",
    "                regularizer=\"LP\",\n",
    "                regularizer_params={'p':3, 'lambda':1e-5},\n",
    "                seed=0,\n",
    "                verbose=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# load knowledge graph triples\n",
    "triples = []\n",
    "triples_df = pd.read_csv(\".../kg_demo_aug2021.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "\n",
    "triples = triples_df.to_numpy()\n",
    "\n",
    "#training and saving\n",
    "from ampligraph.latent_features import save_model\n",
    "\n",
    "model_transe.fit(triples)\n",
    "save_model(model_transe, '.../kg_all_model_transe_aug2021.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472ea84",
   "metadata": {},
   "source": [
    "The code in this tutorial can be applied to any of the other built-in ampligraph embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e7f83",
   "metadata": {},
   "source": [
    "## Obtaining the Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67814b0",
   "metadata": {},
   "source": [
    "We now will need to seperate our CPE-CVE and CVE-CWE triples into removed and non-removed triples. First, load in our triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d284122",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_cpe2cve = []\n",
    "kg_cve2cwe = []\n",
    "\n",
    "kg_cpe2cve_df = pd.read_csv(\".../cpe2cve-aug2021.csv\")\n",
    "for i,r in kg_cpe2cve_df.iterrows():\n",
    "    kg_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "kg_cve2cwe_df = pd.read_csv(\".../cve2cwe-aug2021.csv\")\n",
    "for i,r in kg_cve2cwe_df.iterrows():\n",
    "    kg_cve2cwe.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "kg_cpe2cve_nov2022 = []\n",
    "\n",
    "kg_cpe2cve_nov2022_df = pd.read_csv(\".../cpe2cve-nov2022.csv\")\n",
    "for i,r in kg_cpe2cve_nov2022_df.iterrows():\n",
    "    kg_cpe2cve_nov2022.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "kg_cve2cwe_nov2022 = []\n",
    "\n",
    "kg_cve2cwe_2022_df = pd.read_csv(\".../cve2cwe-nov2022.csv\")\n",
    "for i,r in kg_cve2cwe_2022_df.iterrows():\n",
    "    kg_cve2cwe_nov2022.append([r['subject'],r['predicate'],r['object']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01299aef",
   "metadata": {},
   "source": [
    "We now have a list where each element is a list with the 3 strings that make up a triple. For this specific purpose, we will mash the 3 strings into one string to create a 1D list for easier manipulation and string comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b152561",
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = []\n",
    "li2 = []\n",
    "l1 =[]\n",
    "l2 = []\n",
    "for l in kg_cpe2cve_nov2022:\n",
    "    l1.append(l[0]+','+l[1]+','+l[2])\n",
    "for l in kg_cve2cwe_nov2022:\n",
    "    li1.append(l[0]+','+l[1]+','+l[2])\n",
    "for l in kg_cpe2cve:\n",
    "    l2.append(l[0]+','+l[1]+','+l[2])\n",
    "for l in kg_cve2cwe:\n",
    "    li2.append(l[0] + ',' + l[1] + ',' + l[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05366f2",
   "metadata": {},
   "source": [
    "To obtain the removed triples, we then need to find the triples that were in the August 2021 triples but are not in the November 2022 triples. That can be done by subtracting the 1D lists from each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tmp1 = list(set(li2) - set(li1))\n",
    "new_tmp2 = list(set(l2)- set(l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d94a80",
   "metadata": {},
   "source": [
    "We then convert the 1D lists back to a list of list of 3 strings and save them into a seperate `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in new_tmp1:\n",
    "    removed_cwe2cve.append(s.split(','))\n",
    "for s in new_tmp2:\n",
    "    removed_cpe2cve.append(s.split(','))\n",
    "\n",
    "removed_cwe2cve_df = pd.DataFrame(removed_cwe2cve, columns=[\"subject\", \"predicate\", \"object\"]).sort_values(by=['subject'], ascending=True).reset_index(drop=True)\n",
    "removed_cwe2cve_df.to_csv('.../removed_cwe2cve.csv', index=False)\n",
    "\n",
    "removed_cpe2cve_df = pd.DataFrame(removed_cpe2cve, columns=[\"subject\", \"predicate\", \"object\"]).sort_values(by=['subject'], ascending=True).reset_index(drop=True)\n",
    "removed_cpe2cve_df.to_csv('.../removed_cpe2cve.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd14e0",
   "metadata": {},
   "source": [
    "The non-removed triples can be obtained with similiar logic. We just need to subtract the removed triples from the August 2021 triples to get the triples that weren't removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonremoved_cve2cwe_temp = list(set(li2) - set(new_tmp1))\n",
    "nonremoved_cpe2cve_temp = list(set(l2) - set(new_tmp2))\n",
    "\n",
    "nonremoved_cwe2cve = []\n",
    "nonremoved_cpe2cve = []\n",
    "\n",
    "for s in nonremoved_cve2cwe_temp:\n",
    "    nonremoved_cwe2cve.append(s.split(','))\n",
    "for s in nonremoved_cpe2cve_temp:\n",
    "    nonremoved_cpe2cve.append(s.split(','))\n",
    "\n",
    "nonremoved_cwe2cve_df = pd.DataFrame(nonremoved_cwe2cve, columns=[\"subject\", \"predicate\", \"object\"]).sort_values(by=['subject'], ascending=True).reset_index(drop=True)\n",
    "nonremoved_cwe2cve_df.to_csv('.../nonremoved_cwe2cve.csv', index=False)\n",
    "\n",
    "nonremoved_cpe2cve_df = pd.DataFrame(nonremoved_cpe2cve, columns=[\"subject\", \"predicate\", \"object\"]).sort_values(by=['subject'], ascending=True).reset_index(drop=True)\n",
    "nonremoved_cpe2cve_df.to_csv('.../nonremoved_cpe2cve.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f556f",
   "metadata": {},
   "source": [
    "Since the number of non-removed triples significantly outnumber the number of removed triples, I chose to only evaluate a sample of the non-removed triples so that the plots we generate later would be easier to see. I arbituarly chose 5% as the the percentage of non-removed triples to include, but for more realistic results, these evaulations can also be done with all the non-removed triples as the testing sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79543124",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_fraction = 0.05\n",
    "\n",
    "\n",
    "pos_cwe2cve_triples_testing = random.sample(nonremoved_cwe2cve, int(len(nonremoved_cwe2cve)*testing_fraction))\n",
    "pos_cpe2cve_triples_testing = random.sample(nonremoved_cpe2cve, int(len(nonremoved_cpe2cve)*testing_fraction))\n",
    "\n",
    "pos_cwe2cve_triples_testing_df = pd.DataFrame(pos_cwe2cve_triples_testing, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "pos_cwe2cve_triples_testing_df.to_csv('.../pos_cwe2cve_triples_testing.csv', index=False)\n",
    "\n",
    "pos_cpe2cve_triples_testing_df = pd.DataFrame(pos_cpe2cve_triples_testing, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "pos_cpe2cve_triples_testing_df.to_csv('.../pos_cpe2cve_triples_testing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52db3df",
   "metadata": {},
   "source": [
    "## Evaulating Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c5a35",
   "metadata": {},
   "source": [
    "Each embedding model has a scoring function, where one can feed a triple into the model and the model produces a score which determines how likely that triple is to actually exist or not. Lower scores indicate a triple is less likely to exist, and vice versa for higher scores. We will use our embedding model's scoring function to see if there is any difference in behavior between removed triples and non-removed triples.\n",
    "\n",
    "First, starting with CPE-CVE triples, we load in our datasets and model in order to calculate score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bebab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in triples\n",
    "pos_test_cpe2cve = []\n",
    "pos_test_cpe2cve_df = pd.read_csv(\".../pos_cpe2cve_triples_testing.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in pos_test_cpe2cve_df.iterrows():\n",
    "    pos_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "    \n",
    "neg_test_cpe2cve = []\n",
    "neg_test_cpe2cve_df = pd.read_csv(\".../removed_cpe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in neg_test_cpe2cve_df.iterrows():\n",
    "    neg_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "    \n",
    "from ampligraph.latent_features import restore_model\n",
    "model_transe = restore_model('.../kg_all_model_transe_aug2021.pkl')\n",
    "\n",
    "#Calculate scores\n",
    "test = np.array(pos_test_cpe2cve + neg_test_cpe2cve)\n",
    "scores_cpe2cve = model_transe.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d89c3",
   "metadata": {},
   "source": [
    "With score, some typical performance metrics that use the ground truth label are precision, recall, and F1-score (which balances precision and recall as they are typically inversly related). We then need to create a list of ground-truth labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pos_cpe2cve = []\n",
    "\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(1)\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(0)\n",
    "\n",
    "is_pos_cpe2cve = np.asarray(is_pos_cpe2cve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab45172",
   "metadata": {},
   "source": [
    "The function we are using, `precision_recall_curve()` from the sklearn package, finds a threshold with the best F1-score for positive triples, where the triples above the threshold are labeled positive and the triples below are labeled negative. An issue arises if we attempt to find the best threshold for non-removed (considered positive here) triples' F1-score, where the best performance is obtained by simply labeling literally every triple as positive, which obviously is not useful.\n",
    "\n",
    "Instead, I looked to find the best threshold by treating our removed triples as positive. However, `precision_recall_curve()` always labels values above a threshold as positive and values below as negative when testing the different thresholds. This wouldn't work since we expect that the removed triples (that we are now treating as positive) will tend to have lower scores than the non-removed ones. \n",
    "\n",
    "To get around this, I simply inverted the scores, multiplying all them by -1. This would result in the removed triples tending towards higher scores than the non-removed ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#invert the scores\n",
    "inverse_scores_cpe2cve = [];\n",
    "\n",
    "for s in scores_cpe2cve:\n",
    "    inverse_scores_cpe2cve.append(-s);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32954e",
   "metadata": {},
   "source": [
    "We can now find the best F1-score threshold, and print our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_cpe2cve = 2 * recall_cpe2cve * precision_cpe2cve\n",
    "denom_cpe2cve = recall_cpe2cve + precision_cpe2cve\n",
    "f1_scores_cpe2cve = np.divide(numerator_cpe2cve, denom_cpe2cve, out=np.zeros_like(denom_cpe2cve), where=(denom_cpe2cve!=0))\n",
    "\n",
    "print(\"Results:\")\n",
    "print('CPE to CVE, Scores Best threshold: ', -thresholds_cpe2cve[np.argmax(f1_scores_cpe2cve)])\n",
    "print('CPE to CVE, Scores Rank Precision and recall: ', precision_cpe2cve[np.argmax(f1_scores_cpe2cve)], recall_cpe2cve[np.argmax(f1_scores_cpe2cve)])\n",
    "print('CPE to CVE, Scores Rank Best F1-Score: ', np.max(f1_scores_cpe2cve))\n",
    "print()\n",
    "\n",
    "#Plotting\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall_cpe2cve[50:-10], precision_cpe2cve[50:-10])\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(fontfamily = 'Times New Roman', fontsize=18)\n",
    "plt.yticks(fontfamily = 'Times New Roman', fontsize=18)\n",
    "plt.ylabel('Precision', fontfamily = 'Times New Roman', fontsize = 18)\n",
    "plt.xlabel('Recall', fontfamily = 'Times New Roman', fontsize = 18)\n",
    "plt.title('CPE to CVE')\n",
    "plt.grid(b=True, which='major', color='#999999', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7df4c",
   "metadata": {},
   "source": [
    "To futher understand the behavior of the triples, we can create a simple histogram of our triples' scores. I also included our best F1-score threshold as a reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a054a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cpe2cve_pos = sorted(model_transe.predict(pos_test_cpe2cve))\n",
    "scores_cpe2cve_neg = sorted(model_transe.predict(neg_test_cpe2cve))\n",
    "bins = np.linspace(-10, 0, 50)\n",
    "\n",
    "y1, x1, _ = plt.hist(scores_cpe2cve_pos, bins, alpha=0.5, label='Non-Removed')\n",
    "y2, x2, _ = plt.hist(scores_cpe2cve_neg, bins, alpha=0.5, label='Removed')\n",
    "thresh_cpe2cve = -thresholds_cpe2cve[np.argmax(f1_scores_cpe2cve)]\n",
    "plt.plot([thresh_cpe2cve, thresh_cpe2cve], [0,max([y1.max(), y2.max()])], color='k', label = 'Threshold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE')\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c775859",
   "metadata": {},
   "source": [
    "Another way to visualize the scores is to see what percentage of triples are below each threshold. To do that, we need to calculate the percentile at each threshold:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f790ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cpe2cve_pos_percentile = []\n",
    "current_count = 0\n",
    "current_threshold_index = 0\n",
    "thresholds_cpe2cve_flipped = np.negative(np.flip(thresholds_cpe2cve))\n",
    "for thresh in thresholds_cpe2cve_flipped:\n",
    "    while(current_count < len(scores_cpe2cve_pos)):\n",
    "        if(scores_cpe2cve_pos[current_count] >= thresh):\n",
    "            current_percentile = 100 * (current_count/len(scores_cpe2cve_pos))\n",
    "            scores_cpe2cve_pos_percentile.append(current_percentile)\n",
    "            break\n",
    "        current_count += 1\n",
    "\n",
    "while(len(scores_cpe2cve_pos_percentile) < len(thresholds_cpe2cve_flipped)):\n",
    "    scores_cpe2cve_pos_percentile.append(100)\n",
    "    \n",
    "        \n",
    "scores_cpe2cve_neg_percentile = []\n",
    "current_count = 0\n",
    "current_threshold_index = 0\n",
    "for thresh in thresholds_cpe2cve_flipped:\n",
    "    while(current_count < len(scores_cpe2cve_neg)):\n",
    "        if(scores_cpe2cve_neg[current_count] >= thresh):\n",
    "            current_percentile = 100 * (current_count/len(scores_cpe2cve_neg))\n",
    "            scores_cpe2cve_neg_percentile.append(current_percentile)\n",
    "            break\n",
    "        current_count += 1\n",
    "\n",
    "while(len(scores_cpe2cve_neg_percentile) < len(thresholds_cpe2cve_flipped)):\n",
    "    scores_cpe2cve_neg_percentile.append(100)\n",
    "\n",
    "plt.ylim(0, 100)\n",
    "plt.plot(thresholds_cpe2cve_flipped, scores_cpe2cve_pos_percentile, label = \"Non-Removed\") \n",
    "plt.plot(thresholds_cpe2cve_flipped, scores_cpe2cve_neg_percentile, label = \"Removed\") \n",
    "plt.plot([thresh_cpe2cve, thresh_cpe2cve], [0,100], color='k', alpha=0.5, label = 'Best F1 Threshold')\n",
    "plt.title('CPE to CVE Percentile')\n",
    "plt.xlabel(\"Threshold (score)\")\n",
    "plt.ylabel(\"Percentile Below Threshold\")\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68257d08",
   "metadata": {},
   "source": [
    "Since our ability to seperate the removed and non-removed triples was not particularly impressive, I wanted to compare how aritifically generated negatives compared to removed triples. I generated one artifical triple per each non-removed triple by replacing the cpe element with a random cpe element. \n",
    "\n",
    "We will also need `cpelist_connected.txt` from the original GitHub to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load kg_cpe2cve and kg_cve2cwe\n",
    "kg_cpe2cve = []\n",
    "\n",
    "kg_cpe2cve_df = pd.read_csv(\".../cpe2cve-aug2021.csv\")\n",
    "for i,r in kg_cpe2cve_df.iterrows():\n",
    "    kg_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "# load connected_cpelist\n",
    "f = open('.../cpelist_connected.txt', 'r')\n",
    "connected_cpelist = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "import random\n",
    "\n",
    "neg_test_cpe2cve_generated = []\n",
    "\n",
    "dict_cpe2cve = {} # cve as key, replace cpe\n",
    "dict_cve2cpe = {} # cpe as key, replace cve\n",
    "\n",
    "cpe2cve_alltime = kg_cpe2cve\n",
    "\n",
    "for i in range(len(cpe2cve_alltime)):\n",
    "    if cpe2cve_alltime[i][2] not in dict_cpe2cve.keys():\n",
    "        dict_cpe2cve[cpe2cve_alltime[i][2]] = []\n",
    "    dict_cpe2cve[cpe2cve_alltime[i][2]].append(cpe2cve_alltime[i][0])\n",
    "\n",
    "    if cpe2cve_alltime[i][0] not in dict_cve2cpe.keys():\n",
    "        dict_cve2cpe[cpe2cve_alltime[i][0]] = []\n",
    "    dict_cve2cpe[cpe2cve_alltime[i][0]].append(cpe2cve_alltime[i][2])\n",
    "\n",
    "test_cves = []\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    if pos_test_cpe2cve[i][2] not in test_cves:\n",
    "        test_cves.append(pos_test_cpe2cve[i][2])\n",
    "\n",
    "for v in test_cves:\n",
    "    rnd_lst = []\n",
    "    while len(rnd_lst) < 1:\n",
    "        rnd_cpe = random.choice(connected_cpelist)\n",
    "        if rnd_cpe not in dict_cpe2cve[v]:\n",
    "            rnd_lst.append(rnd_cpe)\n",
    "    for p in rnd_lst:\n",
    "        neg_test_cpe2cve_generated.append([p,'MatchingCVE',v])\n",
    "        \n",
    "# save generated negative CPE-CVE triples locally\n",
    "neg_test_cpe2cve_generated_df = pd.DataFrame(neg_test_cpe2cve_generated, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "neg_test_cpe2cve_generated_df.to_csv('.../neg_test_cpe2cve_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a3a2d",
   "metadata": {},
   "source": [
    "We then need to rebuild our ground truth label list to include the labels for the arictifial negatives, before scoring and plotting like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416397b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label list based on removed, non-removed, and arifiticial triples\n",
    "is_pos_cpe2cve = []\n",
    "\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(1)\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(0)\n",
    "for i in range(len(neg_test_cpe2cve_generated)):\n",
    "    is_pos_cpe2cve.append(0)\n",
    "\n",
    "is_pos_cpe2cve = np.asarray(is_pos_cpe2cve)\n",
    "\n",
    "\n",
    "#Calculate Scores\n",
    "test = np.array(pos_test_cpe2cve + neg_test_cpe2cve + neg_test_cpe2cve_generated)\n",
    "scores_cpe2cve = model_transe.predict(test)\n",
    "\n",
    "#invert scores\n",
    "inverse_scores_cpe2cve = [];\n",
    "for s in scores_cpe2cve:\n",
    "    inverse_scores_cpe2cve.append(-s);\n",
    "\n",
    "#Calculate F1-scores based on label list and TransE score\n",
    "precision_cpe2cve, recall_cpe2cve, thresholds_cpe2cve = precision_recall_curve(is_pos_cpe2cve, inverse_scores_cpe2cve, pos_label=0)\n",
    "\n",
    "#Plotting\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall_cpe2cve[50:-10], precision_cpe2cve[50:-10])\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(fontfamily = 'Times New Roman', fontsize=18)\n",
    "plt.yticks(fontfamily = 'Times New Roman', fontsize=18)\n",
    "plt.ylabel('Precision', fontfamily = 'Times New Roman', fontsize = 18)\n",
    "plt.xlabel('Recall', fontfamily = 'Times New Roman', fontsize = 18)\n",
    "plt.title('CPE to CVE')\n",
    "plt.grid(b=True, which='major', color='#999999', linestyle='-')\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "#printing precision recall results \n",
    "numerator_cpe2cve = 2 * recall_cpe2cve * precision_cpe2cve\n",
    "denom_cpe2cve = recall_cpe2cve + precision_cpe2cve\n",
    "f1_scores_cpe2cve = np.divide(numerator_cpe2cve, denom_cpe2cve, out=np.zeros_like(denom_cpe2cve), where=(denom_cpe2cve!=0))\n",
    "\n",
    "print('Results:')\n",
    "print('CPE to CVE, Scores Best threshold: ', -thresholds_cpe2cve[np.argmax(f1_scores_cpe2cve)])\n",
    "print('CPE to CVE, Scores Rank Precision and recall: ', precision_cpe2cve[np.argmax(f1_scores_cpe2cve)], recall_cpe2cve[np.argmax(f1_scores_cpe2cve)])\n",
    "print('CPE to CVE, Scores Rank Best F1-Score: ', np.max(f1_scores_cpe2cve))\n",
    "print()\n",
    "\n",
    "#plotting histogram\n",
    "scores_cpe2cve_pos = model_transe.predict(pos_test_cpe2cve);\n",
    "scores_cpe2cve_neg = model_transe.predict(neg_test_cpe2cve)\n",
    "scores_cpe2cve_neg_generated = model_transe.predict(neg_test_cpe2cve_generated)\n",
    "bins = np.linspace(-20, 0, 50)\n",
    "\n",
    "plt.figure()\n",
    "y1, x1, _ = plt.hist(scores_cpe2cve_pos, bins, alpha=0.5, label='Non-Removed')\n",
    "y2, x2, _ = plt.hist(scores_cpe2cve_neg, bins, alpha=0.5, label='Removed')\n",
    "y3, x3, _ = plt.hist(scores_cpe2cve_neg_generated, bins, alpha=0.5, label='Generated negs')\n",
    "thresh_cpe2cve = -thresholds_cpe2cve[np.argmax(f1_scores_cpe2cve)]\n",
    "plt.plot([thresh_cpe2cve, thresh_cpe2cve], [0,max([y1.max(), y2.max(), y3.max()])], color='k', label = 'Threshold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('CPE to CVE')\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35906f6c",
   "metadata": {},
   "source": [
    "All this score evaluation can also be done for CVE-CWE triples with the same methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37533eee",
   "metadata": {},
   "source": [
    "## Evaluating Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b2e49",
   "metadata": {},
   "source": [
    "We can also run a similiar evaluation using rank. We have a ranking function that replaces either the subject or object with a bunch of random matching types, generating several artificial negatives, and then ranks how the original triple performs relative to the artificial triples. Since this rank is just based on score, we don't expect any improvement but I still provide my code for testing this.\n",
    "\n",
    "When calculating a best F1-score threshold fo rank, we actually expect higher rank for removed triples compared to non-removed triples. Thus, we don't actually have to invert the ranks like we did for scores when we treat the removed triples as positives. If we were to treat non-removed triples as positive, then the inversion would be necessary again.\n",
    "\n",
    "First, we use ampligraph's `evaluate_performance()` function to calculate the ranks of our triples. I decided not to do random sampling for the non-removed triples set to simplify things, but you certainly could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183edfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in positive and negative validation triples\n",
    "pos_test_cwe2cve = []\n",
    "pos_test_cwe2cve_df = pd.read_csv(\".../nonremoved_cwe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in pos_test_cwe2cve_df.iterrows():\n",
    "    pos_test_cwe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "neg_test_cwe2cve = []\n",
    "neg_test_cwe2cve_df = pd.read_csv(\".../removed_cwe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in neg_test_cwe2cve_df.iterrows():\n",
    "    neg_test_cwe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "\n",
    "pos_test_cpe2cve = []\n",
    "pos_test_cpe2cve_df = pd.read_csv(\".../nonremoved_cpe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in pos_test_cpe2cve_df.iterrows():\n",
    "    pos_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "neg_test_cpe2cve = []\n",
    "neg_test_cpe2cve_df = pd.read_csv(\".../removed_cpe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in neg_test_cpe2cve_df.iterrows():\n",
    "    neg_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "#Creating validation triple sets\n",
    "X_test_cwe2cve_ordered = np.array(pos_test_cwe2cve + neg_test_cwe2cve);\n",
    "X_test_cpe2cve_ordered = np.array(pos_test_cpe2cve + neg_test_cpe2cve);\n",
    "\n",
    "#Filter triples\n",
    "triples = pd.read_csv(\".../kg_demo_aug2021.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "filter_triples = triples.to_numpy()\n",
    "\n",
    "#Model evaluation\n",
    "from ampligraph.latent_features import restore_model\n",
    "model_transe = restore_model('.../kg_all_model_transe_aug2021.pkl')\n",
    "\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "\n",
    "cpe2cve_ranks = evaluate_performance(X_test_cpe2cve_ordered,\n",
    "                              model=model_transe,\n",
    "                              filter_triples=filter_triples,\n",
    "                              verbose=True);\n",
    "\n",
    "cpe2cve_ranks_df = pd.DataFrame(cpe2cve_ranks, columns=[\"subject\", \"object\"])\n",
    "cpe2cve_ranks_df.to_csv('.../cpe2cve_ranks.csv', index=False)\n",
    "\n",
    "cwe2cve_ranks = evaluate_performance(X_test_cwe2cve_ordered,\n",
    "                              model=model_transe,\n",
    "                              filter_triples=filter_triples,\n",
    "                              verbose=True);\n",
    "\n",
    "cwe2cve_ranks_df = pd.DataFrame(cwe2cve_ranks, columns=[\"subject\", \"object\"])\n",
    "cwe2cve_ranks_df.to_csv('.../cwe2cve_ranks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c8192",
   "metadata": {},
   "source": [
    "The ranking function replaces either the subject or object with a bunch of random matching types, generating several artificial negatives, and then compares how the original triple performs relative to the artificial triples. For each triple then, we get 2 ranks: the rank compared to generated negatives from randomly replacing the subject, and the rank compared to generated negatives from randomly replacing the object. \n",
    "\n",
    "Starting with CPE-CVE triples, I first evaulated the performance if the subject (CPE) was replaced in generating the aritfical negatives. I followed the same process as with score for finding the best F1-score threshold and plotting a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in ranks and triples\n",
    "cpe2cve_ranks = []\n",
    "cpe2cve_ranks_df = pd.read_csv('.../cpe2cve_ranks_all.csv',usecols = [\"subject\", \"object\"])\n",
    "for i,r in cpe2cve_ranks_df.iterrows():\n",
    "    cpe2cve_ranks.append([r['subject'],r['object']])\n",
    "    \n",
    "pos_test_cpe2cve = []\n",
    "pos_test_cpe2cve_df = pd.read_csv(\".../nonremoved_cpe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in pos_test_cpe2cve_df.iterrows():\n",
    "    pos_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "    \n",
    "neg_test_cpe2cve = []\n",
    "neg_test_cpe2cve_df = pd.read_csv(\".../removed_cpe2cve.csv\", usecols=[\"subject\", \"predicate\", \"object\"])\n",
    "for i,r in neg_test_cpe2cve_df.iterrows():\n",
    "    neg_test_cpe2cve.append([r['subject'],r['predicate'],r['object']])\n",
    "\n",
    "\n",
    "#Label list based on 'pos_test_cpe2cve' and 'neg_test_cpe2cve'\n",
    "is_pos_cpe2cve = []\n",
    "\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(1)\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    is_pos_cpe2cve.append(0)\n",
    "\n",
    "is_pos_cpe2cve = np.asarray(is_pos_cpe2cve)\n",
    "\n",
    "#Get the ranks of the first column\n",
    "cperank_cpe2cve = np.transpose(cpe2cve_ranks)[0]\n",
    "\n",
    "#Calculate F1-scores based on label list and ranks of first column (subject/CPE)\n",
    "precision, recall, thresholds = precision_recall_curve(is_pos_cpe2cve, cperank_cpe2cve, pos_label=0)\n",
    "\n",
    "numerator = 2 * recall * precision\n",
    "denom = recall + precision\n",
    "f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "\n",
    "print('Results')\n",
    "print('CPE to CVE, CPE Rank Best threshold: ', thresholds[np.argmax(f1_scores)])\n",
    "print('CPE to CVE, CPE Rank Precision and recall: ', precision[np.argmax(f1_scores)], recall[np.argmax(f1_scores)])\n",
    "print('CPE to CVE, CPE Rank Best F1-Score: ', np.max(f1_scores))\n",
    "print()\n",
    "\n",
    "pos_cperank_cpe2cve = []\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    pos_cperank_cpe2cve.append(cperank_cpe2cve[i])\n",
    "\n",
    "temp = i + 1;\n",
    "neg_cperank_cpe2cve = []\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    neg_cperank_cpe2cve.append(cperank_cpe2cve[i + temp])\n",
    "\n",
    "mr = mr_score(pos_cperank_cpe2cve)\n",
    "mrr = mrr_score(pos_cperank_cpe2cve)\n",
    "\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(pos_cperank_cpe2cve, n=20)\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(pos_cperank_cpe2cve, n=10)\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(pos_cperank_cpe2cve, n=3)\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(pos_cperank_cpe2cve, n=1)\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()\n",
    "\n",
    "mr = mr_score(neg_cperank_cpe2cve)\n",
    "mrr = mrr_score(neg_cperank_cpe2cve)\n",
    "\n",
    "print(\"Removed CPE to CVE, CPE Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Removed CPE to CVE, CPE Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(neg_cperank_cpe2cve, n=20)\n",
    "print(\"Removed CPE to CVE, CPE Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(neg_cperank_cpe2cve, n=10)\n",
    "print(\"Removed CPE to CVE, CPE Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(neg_cperank_cpe2cve, n=3)\n",
    "print(\"Removed CPE to CVE, CPE Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(neg_cperank_cpe2cve, n=1)\n",
    "print(\"Removed CPE to CVE, CPE Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()\n",
    "print()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "y1, x1, _ = plt.hist(pos_cperank_cpe2cve, bins, alpha=0.5, label='Non-Removed')\n",
    "y2, x2, _ = plt.hist(neg_cperank_cpe2cve, bins, alpha=0.5, label='Removed')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.plot([thresh, thresh], [0,max([y1.max(), y2.max()])], color='k', label = 'Threshold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CPE Rank')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "y1, x1, _ = plt.hist(pos_cperank_cpe2cve, bins, color='lightblue')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.plot([thresh, thresh], [0, y1.max()], color='k', label = 'Threshold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CPE Rank Non-Removed')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "y1, x1, _ = plt.hist(neg_cperank_cpe2cve, bins, color='orange')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.plot([thresh, thresh], [0, y1.max()], color='k', label = 'Threshold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CPE Rank Removed')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b437f",
   "metadata": {},
   "source": [
    "I then evaulate the performance for if the object (CVE) was replaced in generating the aritfical negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "###Calculate F1-scores based on label list and ranks of second column (Object/CVE)\n",
    "###\n",
    "cverank_cpe2cve = np.transpose(cpe2cve_ranks)[1]\n",
    "precision, recall, thresholds = precision_recall_curve(is_pos_cpe2cve, cverank_cpe2cve, pos_label=0)\n",
    "\n",
    "numerator = 2 * recall * precision\n",
    "denom = recall + precision\n",
    "f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "\n",
    "print('CPE to CVE, CVE Rank Best threshold: ', thresholds[np.argmax(f1_scores)])\n",
    "print('CPE to CVE, CVE Rank Precision and recall: ', precision[np.argmax(f1_scores)], recall[np.argmax(f1_scores)])\n",
    "print('CPE to CVE, CVE Rank Best F1-Score: ', np.max(f1_scores))\n",
    "print()\n",
    "\n",
    "pos_cverank_cpe2cve = []\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    pos_cverank_cpe2cve.append(cverank_cpe2cve[i])\n",
    "\n",
    "temp = i + 1;\n",
    "neg_cverank_cpe2cve = []\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    neg_cverank_cpe2cve.append(cverank_cpe2cve[i + temp])\n",
    "\n",
    "mr = mr_score(pos_cverank_cpe2cve)\n",
    "mrr = mrr_score(pos_cverank_cpe2cve)\n",
    "\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(pos_cverank_cpe2cve, n=20)\n",
    "print(\"Nonremoved CPE to CVE, CPE Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(pos_cverank_cpe2cve, n=10)\n",
    "print(\"Nonremoved CPE to CVE, CVE Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(pos_cverank_cpe2cve, n=3)\n",
    "print(\"Nonremoved CPE to CVE, CVE Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(pos_cverank_cpe2cve, n=1)\n",
    "print(\"Nonremoved CPE to CVE, CVE Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()\n",
    "\n",
    "mr = mr_score(neg_cverank_cpe2cve)\n",
    "mrr = mrr_score(neg_cverank_cpe2cve)\n",
    "\n",
    "print(\"Removed CPE to CVE, CVE Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Removed CPE to CVE, CVE Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(neg_cverank_cpe2cve, n=20)\n",
    "print(\"Removed CPE to CVE, CVE Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(neg_cverank_cpe2cve, n=10)\n",
    "print(\"Removed CPE to CVE, CVE Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(neg_cverank_cpe2cve, n=3)\n",
    "print(\"Removed CPE to CVE, CVE Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(neg_cverank_cpe2cve, n=1)\n",
    "print(\"Removed CPE to CVE, CVE Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()\n",
    "print()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "\n",
    "y1, x1, _ = plt.hist(pos_cverank_cpe2cve, bins, alpha=0.5, label='Non-Removed')\n",
    "y2, x2, _ = plt.hist(neg_cverank_cpe2cve, bins, alpha=0.5, label='Removed')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CVE Rank')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "y1, x1, _ = plt.hist(pos_cverank_cpe2cve, bins, color='lightblue')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CVE Rank Non-Removed')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "bins = np.linspace(1, 10, 10)\n",
    "y1, x1, _ = plt.hist(neg_cverank_cpe2cve, bins, color='orange')\n",
    "thresh = thresholds[np.argmax(f1_scores)]\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('CPE to CVE, CVE Rank Removed')\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a7eb3",
   "metadata": {},
   "source": [
    "Finally, `evaluate_performance()` is capable of handling both object and subject ranks at the same time, but I wasn't able to find the details of how this works and so I wasn't able to generate a histogram to visualize our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cpe2cve_ranks = []\n",
    "for i in range(len(pos_test_cpe2cve)):\n",
    "    pos_cpe2cve_ranks.append(cpe2cve_ranks[i])\n",
    "\n",
    "temp = i + 1;\n",
    "neg_cpe2cve_ranks = []\n",
    "for i in range(len(neg_test_cpe2cve)):\n",
    "    neg_cpe2cve_ranks.append(cpe2cve_ranks[i + temp])\n",
    "\n",
    "mr = mr_score(pos_cpe2cve_ranks)\n",
    "mrr = mrr_score(neg_cpe2cve_ranks)\n",
    "\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(pos_cverank_cpe2cve, n=20)\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(pos_cverank_cpe2cve, n=10)\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(pos_cverank_cpe2cve, n=3)\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(pos_cverank_cpe2cve, n=1)\n",
    "print(\"Nonremoved CPE to CVE, Overall Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()\n",
    "\n",
    "mr = mr_score(neg_cpe2cve_ranks)\n",
    "mrr = mrr_score(neg_cpe2cve_ranks)\n",
    "\n",
    "print(\"Removed CPE to CVE, Overall Rank MRR: %.3f\" % (mrr))\n",
    "print(\"Removed CPE to CVE, Overall Rank MR: %.3f\" % (mr))\n",
    "\n",
    "hits_20 = hits_at_n_score(neg_cverank_cpe2cve, n=20)\n",
    "print(\"Removed CPE to CVE, Overall Rank Hits@20: %.3f\" % (hits_20))\n",
    "hits_10 = hits_at_n_score(neg_cverank_cpe2cve, n=10)\n",
    "print(\"Removed CPE to CVE, Overall Rank Hits@10: %.3f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(neg_cverank_cpe2cve, n=3)\n",
    "print(\"Removed CPE to CVE, Overall Rank Hits@3: %.3f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(neg_cverank_cpe2cve, n=1)\n",
    "print(\"Removed CPE to CVE, Overall Rank Hits@1: %.3f\" % (hits_1))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1808f",
   "metadata": {},
   "source": [
    "All this rank evaluation can also be done for CVE-CWE triples with the same methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
